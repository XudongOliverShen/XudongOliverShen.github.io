<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xudong Shen</title>

    <meta name="author" content="Xudong Shen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Xudong Shen
                </p>
                <p style="text-align: center;">
                  I obtained my Ph.D. in AI from National University of Singapore in 2024, where I was advised by Prof. <a href="https://www.comp.nus.edu.sg/cs/people/mohan/">Mohan Kankanhalli</a>.
                  I earned my bachelor's degree from Zhejiang University, China, in 2019.
                  During 2022–2023, I interned at <a href="https://sail.sea.com">Sea AI Lab</a> in Singapore, working with <a href="https://duchao0726.github.io">Chao Du</a> and <a href="https://p2333.github.io">Tianyu Pang</a>. In 2024–2025, I co-founded a venture-backed AI startup.
                </p>
                <p style="text-align: center;">
                  I am passionate about scaling Reinforcement Learning for multi-modal, long-horizon, complex real-world tasks. My earlier work focused on AI fairness, robustness, safety, and governance, where I took an evaluation-driven approach: stress-testing AI systems and developing model improvements.
                </p>
                <p style="text-align:center">
                  <a href="mailto:xudong.shen@u.nus.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV_Xudong_Shen_20260301_public.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=rCZglhYAAAAJ&hl=en&oi=ao">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/oliverrr_shen">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/XudongOliverShen">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/Xudong.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/Xudong.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/2024_diffusion_fairness/4.png" width="100%">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2311.07604">
                  <span class="papertitle">Finetuning Text-to-Image Diffusion Models for Fairness</span>
                </a>
                <br>
                <strong>Xudong Shen</strong>, Chao Du, Tianyu Pang, Min Lin, Yongkang Wong, Mohan Kankanhalli
                <br>
                <em>ICLR</em>, 2024 <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://github.com/sail-sg/finetune-fair-diffusion">Github</a>
                /
                <a href="https://arxiv.org/abs/2311.07604">arXiv</a>
                <p></p>
                <p>
                  We developed a method to optimize diffusion models for any differentiable objective defined on the generated data, where score/noise prediction and RL fail.
                  We applied it to enhance & control output diversity in text-to-image generation.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/2025_multimodal_safety/1.png" width="100%">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2501.10057">
                  <span class="papertitle">MSTS: A Multimodal Safety Test Suite for Vision-Language Models</span>
                </a>
                <br>
                Paul Röttger, ..., <strong>Xudong Shen</strong>, ... (22 authors)
                <br>
                <em>arXiv</em>, 2025
                <br>
                <a href="https://github.com/paul-rottger/msts-multimodal-safety">Github</a>
                /
                <a href="https://arxiv.org/abs/2501.10057">arXiv</a>
                <p></p>
                <p>
                  Image+text prompts trigger more safety failures than text-only.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/2025_SHADES/1.png" width="100%">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://aclanthology.org/2025.naacl-long.600/">
                  <span class="papertitle">SHADES: Towards a Multilingual Assessment of Stereotypes in Large Language Models</span>
                </a>
                <br>
                Margaret Mitchell, ..., <strong>Xudong Shen</strong>, ... (55 authors)
                <br>
                <em>NAACL</em>, 2025
                <br>
                <a href="https://aclanthology.org/2025.naacl-long.600/">paper</a>
                <p></p>
                <p>
                  Probes multilingual stereotypes and its cross-lingual transfer in LLMs.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/2023_inverse_scaling/1.png" width="100%">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2306.09479">
                  <span class="papertitle">Inverse Scaling: When Bigger Isn't Better</span>
                </a>
                <br>
                Ian McKenzie, ..., <strong>Xudong Shen</strong>, ... (26 authors)
                <br>
                <em>TMLR</em>, 2023
                <br>
                <a href="https://github.com/inverse-scaling/prize">Github</a>
                /
                <a href="https://arxiv.org/abs/2306.09479">arXiv</a>

                <p></p>
                <p>
                  Shows when larger models consistently perform worse; analyzes failure modes.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/2023_big_bench/1.png" width="100%">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2206.04615">
                  <span class="papertitle">Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models</span>
                </a>
                <br>
                Aarohi Srivastava, ..., <strong>Xudong Shen</strong>, ... (450 authors)
                <br>
                <em>TMLR</em>, 2023
                <br>
                <a href="https://github.com/google/BIG-bench">Github</a>
                /
                <a href="https://arxiv.org/abs/2206.04615">arXiv</a>
                <p></p>
                <p>
                  Large-scale eval that reveals where LLM capabilities scale well & where they don't.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/2023_NL_Augmenter/1.png" width="100%">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.02721">
                  <span class="papertitle">NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation</span>
                </a>
                <br>
                Kaustubh D Dhole, ..., <strong>Xudong Shen</strong>,... (125 authors)
                <br>
                <em>NEJLT</em>, 2023
                <br>
                <a href="https://github.com/GEM-benchmark/NL-Augmenter">Github</a>
                /
                <a href="https://arxiv.org/abs/2112.02721">arXiv</a>
                <p></p>
                <p>
                  Stress-tested LLM robustness using 100+ natural-language augmentations.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/2022_instruction_tuning/1.png" width="100%">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://aclanthology.org/2022.emnlp-main.340/">
                  <span class="papertitle">Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks</span>
                </a>
                <br>
                Yizhong Wang, ..., <strong>Xudong Shen</strong>, ... (40 authors)
                <br>
                <em>EMNLP</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2204.07705">arXiv</a>
                <p></p>
                <p>
                  Instruction-tuning on 1.6K tasks boosts zero-shot unseen-task performance.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/2023_fair_representation/1.png" width="100%">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/9706306">
                  <span class="papertitle">Fair Representation: Guaranteeing Approximate Multiple Group Fairness for Unknown Tasks</span>
                </a>
                <br>
                <strong>Xudong Shen</strong>, Yongkang Wong, Mohan Kankanhalli
                <br>
                <em>IEEE TPAMI</em>, 2023
                <br>
                <a href="https://github.com/XudongOliverShen/2021-fair-representation">Github</a>
                /
                <a href="https://arxiv.org/abs/2109.00545">arXiv</a>
                <p></p>
                <p>
                  Learns representations with multiple approximate robustness guarantees that transfer to even unseen tasks. We applied it to debias face representations.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/2021_capsule/1.png" width="100%">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2110.00529">
                  <span class="papertitle">Unsupervised Motion Representation Learning with Capsule Autoencoders </span>
                </a>
                <br>
                Ziwei Xu, <strong>Xudong Shen</strong>, Yongkang Wong, Mohan Kankanhalli 
                <br>
                <em>NeurIPS</em>, 2021
                <br>
                <a href="https://github.com/ZiweiXU/CapsuleMotion">Github</a>
                /
                <a href="https://arxiv.org/abs/2110.00529">arXiv</a>
                <p></p>
                <p>
                  Learns representations with built-in interpretability (i.e., part–whole relationships) via capsule networks.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/2023_P2P/1.png" width="100%">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/10.1145/3593013.3594042">
                  <span class="papertitle">Gender Animus Can Still Exist Under Favorable Disparate Impact: a Cautionary Tale from Online P2P Lending</span>
                </a>
                <br>
                <strong>Xudong Shen</strong>, Tianhui Tan, Tuan Q. Phan, Jussi Keppo
                <br>
                <em>FAccT</em>, 2023
                <br>
                <a href="https://dl.acm.org/doi/10.1145/3593013.3594042">paper</a>
                <p></p>
                <p>
                  Time-to-event modeling to predict default & profitability on million-scale lending data.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/2024_CACM/1.jpeg" width="100%">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/10.1145/3653670">
                  <span class="papertitle">Directions of Technical Innovation for Regulatable AI Systems</span>
                </a>
                <br>
                <strong>Xudong Shen</strong>, Hannah Brown, Jiashu Tao, Martin Strobel, Yao Tong, Akshay Narayan, Harold Soh, Finale Doshi-Velez
                <br>
                <em>Communications of the ACM</em>, 2024
                <br>
                <a href="https://dl.acm.org/doi/10.1145/3653670">paper</a>
                <p></p>
                <p>
                  Maps technical mechanisms that make AI easier to regulate in practice.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/2024_DMA/Digital-Markets-Act.png" width="100%">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4769439">
                  <span class="papertitle">Integration of Generative AI in the Digital Markets Act: Contestability and Fairness from a Cross-Disciplinary Perspective</span>
                </a>
                <br>
                Ayse Gizem Yasar, Andrew Chong, Evan Dong, Thomas Krendl Gilbert, Sarah Hladikovat, Roland Maio, Carlos Mougant, <strong>Xudong Shen</strong>, Shubham Singh, Ana-Andreea Stoicat, Savannah Thais
                <br>
                <em>LSE working papers series</em>, 2024
                <br>
                <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4769439">paper</a>
                <p></p>
                <p>
                  Analyzes how Generative AI & foundation models interact with platform regulation.
                </p>
              </td>
            </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a> from <a href="https://jonbarron.info">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
